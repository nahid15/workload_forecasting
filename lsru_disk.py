# -*- coding: utf-8 -*-
"""LSRU_disk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UQC2hYvkEpZRNXViIeepZapJsEO4KfX6
"""

import pandas as pd
import tensorflow as tf
print(tf.__version__)

import numpy as np
import matplotlib.pyplot as plt

"""Definiation of global plot function:"""

def plot_series(time, series, format="-", start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel("Time", fontsize=15)
    plt.ylabel("Disk Usages", fontsize=15)
    plt.grid(True)

"""# Read datasets
Save theme to local location

###For kaggle after uploading dataset use bellow code to *read*
"""

df = pd.read_csv("/kaggle/input/43.csv")

"""##In google colab use mount method or upload method for read dataset
1. upload method

"""

from google.colab import files
uploaded = files.upload()

""" 2. mount method"""

from google.colab import drive
drive.mount('/content/drive')

"""Read CSV"""

df = pd.read_csv("43.csv")
df.to_csv("43.csv")
df

"""Dataset information
* row[0]=time_step; 
* row[4]=Cpu_usages; 
* row[7]=RAM_usage; 
* row[8]=Disk_read; 
* row[9]=Disk_write; 
* row[11]=Bandwidth;

Normalize dataset:
"""

Sum_disk=(df["Disk read throughput [KB/s]"] + df["Disk write throughput [KB/s]"])
df["Disk I/O"]=Sum_disk
df["Disk I/O"]=((df["Disk I/O"]-df["Disk I/O"].min())/(df["Disk I/O"].max()-df["Disk I/O"].min()))*20
df.to_csv("43.csv")
df

"""Read csv and copy datas into lists:"""

import csv
time_step = []
BW_usage = []

with open('43.csv') as csvfile:
  reader = csv.reader(csvfile, delimiter=',')
  next(reader)
  for row in reader:
    BW_usage.append(float(row[12]))
    time_step.append(int(row[0]))

"""# Visualize data set"""

series = np.array(BW_usage)
time = np.array(time_step)
plt.figure(figsize=(10, 6))
#plt.title("Bandwidth Usages dataset",fontsize=16)

plot_series(time, series)
 
plt.savefig('Raw_Data.png', dpi = 720)

"""# *Split dataset as train and validation set*"""

split_time = 7500
time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]

"""Plot test data (not mandetory):"""

plt.figure(figsize=(10, 6))
#plt.title("Test Data",fontsize=16)
plot_series(time_valid, x_valid)
plt.savefig('test_data.png', dpi = 720)

"""# Window function & forcasting funtion"""

window_size = 30
batch_size = 32
shuffle_buffer_size = 1000

#window function defination
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

# forecasting function defination
def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    forecast = model.predict(ds)
    return forecast

"""# Finding Optimal Learning rate using "Huber Loss""""

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
window_size = 64
batch_size = 125
train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
print(train_set)
print(x_train.shape)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=64, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.GRU(64, return_sequences=True),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(15, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 100)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])

"""# Ploting learning rate vs Loss curve"""

# Checking what is in history data
print('\nhistory dict:', history.history)

"""Finding minimum loss:"""

min_loss=min(history.history["loss"])
min_loss_index=history.history["loss"].index(min_loss)
min_lr=(history.history["lr"])[min_loss_index]
min_lr

"""ploting mimnimum loss point"""

plt.figure(figsize=(10, 6))
plt.semilogx(history.history["lr"], history.history["loss"],'r')
plt.axis([1e-8, 3e-2, 0, 50])
plt.xlabel("Learning rate", fontsize=15)
plt.ylabel("Loss", fontsize=15)
#plt.grid()
s='Min_lr: '+str(min_lr)
plt.axvline(x=min_lr, linewidth=2,color="b",ls="--")
plt.annotate(s, xy=(min_lr, min_loss), xytext=(1E-7, 25),arrowprops=dict(facecolor='black', shrink=0.05 ,width=1.0, headwidth=5.0),fontsize=15)
#plt.savefig('lr_vs_loss.png', dpi = 720) 
plt.show()

"""# Training The Model """

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
train_set = windowed_dataset(x_train, window_size=60, batch_size=124, shuffle_buffer=shuffle_buffer_size)
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=65, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.GRU(64, return_sequences=True),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(15, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 10)
])



optimizer = tf.keras.optimizers.SGD(min_lr, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae","mse","msle","mape"])
history = model.fit(train_set,epochs=300)

#minimum mae error
min(history.history["mae"])

"""model summary:"""

model.summary()

"""Forecasting workload"""

rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]

"""visualization of full workload prediction"""

plt.figure(figsize=(10, 6))
#plt.title("Prediction of dataset",fontsize=16)

plot_series(time_valid, x_valid)
plot_series(time_valid, rnn_forecast,'r' )
plt.legend(('Actual workloads', 'Predicted Workloads'),loc='upper right',fontsize=12)
plt.savefig('BW prediction.png', dpi = 720)

"""Visualizing short time workload prediction"""

plt.figure(figsize=(10, 6))
plot_series(time_valid[0:72], x_valid[0:72])
plot_series(time_valid[0:72], rnn_forecast[0:72],'r' )
plt.legend(('Actual workloads', 'Predicted Workloads'),fontsize=12)
plt.savefig('6 hr BW pred.png', dpi = 720)

"""Visualizing medium time ahead workload prediction"""

plt.figure(figsize=(10, 6))
plot_series(time_valid[0:288], x_valid[0:288])
plot_series(time_valid[0:288], rnn_forecast[0:288],'r' )
plt.legend(('Actual workloads', 'Predicted Workloads'),fontsize=12)
plt.savefig('24 hr bw pred.png', dpi = 720)

"""Visualization of long time ahead prediction"""

plt.figure(figsize=(10, 6))
plot_series(time_valid[0:864], x_valid[0:864])
plot_series(time_valid[0:864], rnn_forecast[0:864],'r' )
plt.legend(('Actual workloads', 'Predicted Workloads'),fontsize=12)
plt.savefig('3 day disk pred.png', dpi = 720)

"""# Error output"""

tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()

tf.keras.metrics.mse(x_valid, rnn_forecast).numpy()

tf.keras.metrics.msle(x_valid, rnn_forecast).numpy()

tf.keras.metrics.mape(x_valid, rnn_forecast).numpy()

"""# Epoch vs Loss curve"""

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
valid_set = windowed_dataset(x_valid, window_size=64, batch_size=125, shuffle_buffer=shuffle_buffer_size)
history1 = model.fit(valid_set,epochs=300)

val_loss=history1.history['mae']
loss=history.history['mae']
epoch=range(len(loss))
epochs=range(len(val_loss))

#zoomed_loss = loss[30:]
#zoomed_epoch = range(30,50)

#zoomed_val_loss = loss[30:]
#zoomed_epochs = range(30,50)
plt.figure(figsize=(10, 6))
#plt.xlim(0, 50)
plt.ylim(0.1,0.5 )
plt.plot(epoch, loss, 'r')
#plt.plot(zoomed_epochs, zoomed_loss, 'r')
plt.plot(epochs, val_loss, 'b')
#plt.plot(zoomed_epochs, zoomed_val_loss, 'r')
plt.title('Training loss and testing loss', fontsize=15)
plt.xlabel("Epochs", fontsize=15)
plt.ylabel("Loss", fontsize=15)
plt.legend(['Train', 'Test'], fontsize=15)#, loc='upper left')
plt.grid()
plt.savefig('train vs test mae.png', dpi = 720) 
plt.figure()
#zoomed_loss = loss[200:]
#zoomed_epochs = range(200,300)

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
loss=history.history['loss']

epochs=range(len(loss)) # Get number of epochs


#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, loss, 'r')
plt.title('Training loss', fontsize=15)
plt.xlabel("Epochs", fontsize=15)
plt.ylabel("Loss", fontsize=15)
plt.legend(["Loss"])
plt.savefig('epoch_v_losss.png', dpi = 720) 
plt.figure()



zoomed_loss = loss[150:]
zoomed_epochs = range(150,300)


#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(zoomed_epochs, zoomed_loss, 'r')
plt.title('Training loss', fontsize=15)
plt.xlabel("Epochs", fontsize=15)
plt.ylabel("Loss", fontsize=15)
plt.legend(["Loss"])

plt.figure()